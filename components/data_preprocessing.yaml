# PIPELINE DEFINITION
# Name: data-preprocessing
# Description: Component 2: Data Preprocessing
#              Handles cleaning, scaling, and splitting data into train/test sets
#              
#              Inputs:
#                  dataset_input (Input[Dataset]): Raw dataset from extraction component
#                      - CSV file with features and target variable
#              
#              Outputs:
#                  train_data (Output[Dataset]): Preprocessed training dataset
#                      - 80% of total data (~16,512 samples)
#                      - Features scaled using StandardScaler
#                      - Saved as CSV file
#                  
#                  test_data (Output[Dataset]): Preprocessed test dataset
#                      - 20% of total data (~4,128 samples)
#                      - Features scaled using StandardScaler
#                      - Saved as CSV file
#                  
#                  train_size (int): Number of training samples
#                  test_size (int): Number of test samples
#                  n_features (int): Number of features
# Inputs:
#    dataset_input: system.Dataset
# Outputs:
#    n_features: int
#    test_data: system.Dataset
#    test_size: int
#    train_data: system.Dataset
#    train_size: int
components:
  comp-data-preprocessing:
    executorLabel: exec-data-preprocessing
    inputDefinitions:
      artifacts:
        dataset_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        n_features:
          parameterType: NUMBER_INTEGER
        test_size:
          parameterType: NUMBER_INTEGER
        train_size:
          parameterType: NUMBER_INTEGER
deploymentSpec:
  executors:
    exec-data-preprocessing:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preprocessing
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'scikit-learn==1.3.0' 'numpy==1.24.3'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.15.1' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preprocessing(\n    dataset_input: Input[Dataset],\n   \
          \ train_data: Output[Dataset],\n    test_data: Output[Dataset]\n) -> NamedTuple('PreprocessingOutputs',\
          \ [\n    ('train_size', int), \n    ('test_size', int),\n    ('n_features',\
          \ int)\n]):\n    \"\"\"\n    Component 2: Data Preprocessing\n    Handles\
          \ cleaning, scaling, and splitting data into train/test sets\n\n    Inputs:\n\
          \        dataset_input (Input[Dataset]): Raw dataset from extraction component\n\
          \            - CSV file with features and target variable\n\n    Outputs:\n\
          \        train_data (Output[Dataset]): Preprocessed training dataset\n \
          \           - 80% of total data (~16,512 samples)\n            - Features\
          \ scaled using StandardScaler\n            - Saved as CSV file\n\n     \
          \   test_data (Output[Dataset]): Preprocessed test dataset\n           \
          \ - 20% of total data (~4,128 samples)\n            - Features scaled using\
          \ StandardScaler\n            - Saved as CSV file\n\n        train_size\
          \ (int): Number of training samples\n        test_size (int): Number of\
          \ test samples\n        n_features (int): Number of features\n    \"\"\"\
          \n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\
          \    from sklearn.preprocessing import StandardScaler\n    from collections\
          \ import namedtuple\n\n    print(\"=\" * 70)\n    print(\"COMPONENT 2: DATA\
          \ PREPROCESSING\")\n    print(\"=\" * 70)\n\n    # Load data\n    df = pd.read_csv(dataset_input.path)\n\
          \    print(f\"\u2713 Loaded dataset: {df.shape}\")\n\n    # Check for missing\
          \ values\n    missing = df.isnull().sum().sum()\n    print(f\"\u2713 Missing\
          \ values found: {missing}\")\n    if missing > 0:\n        print(\"  Handling\
          \ missing values...\")\n        df = df.dropna()\n\n    # Separate features\
          \ and target\n    X = df.drop('PRICE', axis=1)\n    y = df['PRICE']\n  \
          \  n_features = X.shape[1]\n\n    print(f\"\u2713 Features shape: {X.shape}\"\
          )\n    print(f\"\u2713 Target shape: {y.shape}\")\n\n    # Split data into\
          \ train/test (80/20 split)\n    X_train, X_test, y_train, y_test = train_test_split(\n\
          \        X, y, \n        test_size=0.2, \n        random_state=42,\n   \
          \     shuffle=True\n    )\n    print(f\"\\n\u2713 Data split completed:\"\
          )\n    print(f\"  - Training samples: {len(X_train):,} (80%)\")\n    print(f\"\
          \  - Test samples: {len(X_test):,} (20%)\")\n\n    # Scale features using\
          \ StandardScaler\n    print(f\"\\n\u2713 Scaling features using StandardScaler...\"\
          )\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
          \    X_test_scaled = scaler.transform(X_test)\n    print(f\"  - Mean: ~0,\
          \ Std: ~1 (standardized)\")\n\n    # Create DataFrames with scaled data\n\
          \    train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n    train_df['PRICE']\
          \ = y_train.values\n\n    test_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n\
          \    test_df['PRICE'] = y_test.values\n\n    # Save processed data\n   \
          \ train_df.to_csv(train_data.path, index=False)\n    test_df.to_csv(test_data.path,\
          \ index=False)\n\n    print(f\"\\n\u2713 Preprocessed data saved:\")\n \
          \   print(f\"  - Training data: {train_data.path}\")\n    print(f\"  - Test\
          \ data: {test_data.path}\")\n    print(\"=\" * 70)\n\n    # Return output\
          \ tuple\n    outputs = namedtuple('PreprocessingOutputs', ['train_size',\
          \ 'test_size', 'n_features'])\n    return outputs(len(train_df), len(test_df),\
          \ n_features)\n\n"
        image: python:3.9
pipelineInfo:
  name: data-preprocessing
root:
  dag:
    outputs:
      artifacts:
        test_data:
          artifactSelectors:
          - outputArtifactKey: test_data
            producerSubtask: data-preprocessing
        train_data:
          artifactSelectors:
          - outputArtifactKey: train_data
            producerSubtask: data-preprocessing
      parameters:
        n_features:
          valueFromParameter:
            outputParameterKey: n_features
            producerSubtask: data-preprocessing
        test_size:
          valueFromParameter:
            outputParameterKey: test_size
            producerSubtask: data-preprocessing
        train_size:
          valueFromParameter:
            outputParameterKey: train_size
            producerSubtask: data-preprocessing
    tasks:
      data-preprocessing:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preprocessing
        inputs:
          artifacts:
            dataset_input:
              componentInputArtifact: dataset_input
        taskInfo:
          name: data-preprocessing
  inputDefinitions:
    artifacts:
      dataset_input:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
  outputDefinitions:
    artifacts:
      test_data:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
      train_data:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
    parameters:
      n_features:
        parameterType: NUMBER_INTEGER
      test_size:
        parameterType: NUMBER_INTEGER
      train_size:
        parameterType: NUMBER_INTEGER
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.1
